---
title: "Лабораторная работа №1. Загрузка данных"
author: "С.А.Суязова (Аксюк), [sa_aksyuk@guu.ru](mailto:sa_aksyuk@guu.ru)"
date: "26 авг, 2021"
tags: "R[^1], r-project, RStudio"  
output:
  word_document:
    reference_docx: ./Шаблон.docx
    toc: yes
---

Ключевые слова: R[^1], r-project, RStudio  

Примеры выполнены R версии 4.1.1, «Kick Things».  

Версия RStudio: 1.4.1717.  

Все ссылки действительны на 8 февраля 2021 г.  

Репозиторий с материалами к курсу: [github.com/aksyuk/R_data_glimpse](https://github.com/aksyuk/R_data_glimpse)   

Файл с макетом кода для этой практики: `.Labs/lab-01_before.R`    



##### Новая страница    




# Лабораторная работа №1: загрузка данных из .csv и с помощью API сайтов   



## Несколько полезных привычек       

\ \ \ \ \ В любой работе есть важные вещи, которые быстро становятся рутиной. В аналитике это работа с данными на начальном этапе, ещё до того, как нам представится случай проявить своё творческое начало в визуализации переменных и в интерпретации того, что мы видим. Часто загрузка данных -- это рутина, но если к этому этапу отнестить неаккуратно, в ходе анализа можно создать себе неприятные проблемы. Поэтому специалисту по работе с данными лучше сразу формировать у себя полезные привычки, которые, к слову, пригодятся не только в R. 

\ \ \ \ \ Одно из преимуществ R как языка обработки данных -- воспроизводимость результатов. Технически, если код описывает всё, что происходит на разных стадиях исследования, от их загрузки до генерации отчёта, то любой пользователь может выполнить его на своём компьютере и получить такой же отчёт. Разумеется, для этого необходимо выполнение ряда условий, в частности, инструкции должны быть универсальными, а источники данных и пакеты, использованные для их обработки, -- открытыми. В этой практике мы рассмотрим несколько вариантов загрузки данных из открытых источников. При этом будем придерживаться нескольких простых правил, которые помогают избежать многих проблем.  

\ \ \ \ \ *Не задавайте явно рабочую директорию*. Пока код не является самодостаточным пакетом или отдельным приложением, мы считаем, что он адресован пользователям, знакомым с азами R. Рабочую директорию адресат задаёт без нашего участия, и её имя и расположение, очевидно, не совпадут с нашими.  

\ \ \ \ \ Однако, *сохраняйте данные в отдельную директорию внутри рабочей*. Это поможет отделить «сырые» данные и сохранить их на случай, если не будет возможности перезагрузить файл. Далее во всех примерах данные загружаются в папку «data» внутри рабочей директории.  

\ \ \ \ \ *Сохраняйте время и дату загрузки*. Это облегчает контроль версий и просто даёт представление о том, как давно всё произошло. Далее в примерах эта информация сохраняется в текстовом файле внутри директории «data».  

\ \ \ \ \ *Снабжайте данные описанием*. Обычно после предварительной обработки, которая может включать переименование столбцов, заполнение пропусков, изменение макета таблицы, файл данных отличается от исходного. В этом случае хорошим тоном будет составить короткий справочник с описанием проделанных трансформаций и с итоговым списком переменных (столбцов), с обязательным указанием их единиц измерения. В англоязычных источниках такой справочник носит название «code book», дословно -- кодовая книга. Его назначение в том, чтобы составленная вами таблица данных не превращалась для стороннего человека в шифровку. Подобными кодовыми книгами, или справочниками, как они будут называться ниже, снабжены все встроенные в R наборы данных. Чтобы убедиться в этом, достаточно вызвать справку по файлу данных, например: `?mtcars` или `?iris`.  

\ \ \ \ \ Наработав собственный опыт, читатель, разумеется, прибавит к этому минимальному списку свои правила, которые, при должной практике, превратятся в полезные привычки. Главная цель здесь, с одной стороны, настроить рабочее окружение под себя, а с другой -- в случае совместной работы над проектом свести необходимость дополнительных разъяснений к минимуму.  



## Загрузка файла .csv из сети    

\ \ \ \ \ **Пример №1**. Первый и самый простой способ получить данные -- загрузить их в виде файла с известного адреса. Загрузим таблицу со статистикой импорта сливочного масла в РФ за 2010-2018 гг. Источник данных -- база UN COMTRADE (<http://comtrade.un.org/data/>). Данные сохранены в репозитории на github.com и доступны по ссылке: <https://raw.githubusercontent.com/aksyuk/R-data/master/COMTRADE/040510-Imp-RF-comtrade.csv>.  

\ \ \ \ \ Создадим директорию `data` внутри рабочей директории с помощью функции `dir.create()` и файл для записи лога загрузок с помощью функции `file.create()`. Чтобы не перезаписывать их при повторных прогонах кода, добавим проверку условия. При загрузке и чтении данных полезны следующие функции R:  

* `file.exists('`*путь_к_файлу*`')` возвращает `TRUE`, если указанный файл существует, и `FALSE` в противном случае;  

* `exists('`*имя_объекта*`')` возвращает `TRUE`, если указанный объект существует в рабочем пространстве R, и `FALSE` в противном случае.  

\ \ \ \ \ Проверка условия существования файла (объекта) перед загрузкой (чтением) существенно экономит время при работе с таблицами большой размерности.  


```r
# создаём директорию для данных, если она ещё не существует:
data.dir <- './data'
if (!file.exists(data.dir)) dir.create(data.dir)
# создаём файл с логом загрузок, если он ещё не существует:
log.filename <- './data/download.log'
if (!file.exists(log.filename)) file.create(log.filename)
```


\ \ \ \ \ На этапе непосредственной загрузки файла используем функции:  

* `download.file(*URL_файла*, *имя_файла_для_сохранения*)` загружает файл и сохраняет под указанным именем. Второй аргумент может быть именем файла, тогда он будет сохранён в рабочую директорию, а также абсолютным или относительным путём к файлу. Мы используем относительную ссылку на директорию с данными: `./data/*имя_файла*`, где точка означает «в текущей (рабочей) директории».  

* `write(*текст_для_записи*, file = *имя_файла*, append = TRUE)` записывает текст в указанный файл. Аргумент `append = TRUE` означает, что новая строка будет добавлена в конец файла.  
  

```r
# адрес файла
fileURL <- 'https://raw.githubusercontent.com/aksyuk/R-data/master/COMTRADE/040510-Imp-RF-comtrade.csv'
dest.file <- './data/040510-Imp-RF-comtrade.csv'
# загружаем файл, если он ещё не существует, и делаем запись о загрузке в лог:
if (!file.exists(dest.file)) {
    # загрузить файл 
    download.file(fileURL, dest.file)
    # сделать запись в лог
    write(paste('Файл', dest.file, 'загружен', Sys.time()), 
          file = log.filename, append = T)
}
```

\ \ \ \ \ Наконец, чтение данных из загруженного файла во фрейм и просмотр содержимого. Помните, что в случае если таблица содержит текстовые переменные, R автоматически сделает их факторами, присвоив каждому уникальному текстовому значению порядковый номер. Если требуется прочесть заранее неизвестную таблицу, нужно запретить такое преобразование, указав в функции `read.csv()` аргумент `stringsAsFactors = FALSE`. Буквально это будет означать: не делать из символьных столбцов факторы.  




```r
# читаем данные из загруженного .csv во фрейм, 
#  если он ещё не существует
if (!exists('DF.import')) {
    DF.import <- read.csv(dest.file, stringsAsFactors = F)    
}
# предварительный просмотр
dim(DF.import)     # размерность таблицы
str(DF.import)     # структура (характеристики столбцов)
head(DF.import)    # первые несколько строк таблицы
tail(DF.import)    # последние несколько строк таблицы

```

\ \ \ \ \ Это простой пример, поскольку загруженная таблица уже очищена от пустых столбцов и приведена к аккуратному виду, то есть:  

* каждая строка содержит одно наблюдение;  

* каждому столбцу соответствует одна переменная;  

* каждый тип наблюдений (объектов) хранится в отдельной таблице.  

\ \ \ \ \ Это список требований к тому, что принято называть «tidy data», или дословно -- «аккуратные данные». Термин предложен Хэдли Уикхэмом в одноимённой статье в 2014 году[^1]. В дополнение к этому, по заголовкам столбцов этого файла можно понять, что за переменные в нём содержатся. Для данных уже написан справочник: <https://github.com/aksyuk/R-data/blob/master/COMTRADE/CodeBook_040510-Imp-RF-comtrade.md>.  

\ \ \ \ \ Подчеркнём, что данные примера №1 уже подверглись предварительной обработке. Мы вернёмся к этому примеру в разделе «Очистка и предобработка данных», чтобы подробно поговорить об этом.  



## Загрузка данных с помощью API  


#### Сайт статистики международной торговли [comtrade.un.org/](https://comtrade.un.org/)  

\ \ \ \ \ Некоторые сайты предоставляют разработчикам API (application programming interface) – интерфейсы программирования приложений. У R есть средства для работы с API: twitter.com (пакет «twitteR» [^6]), facebook.com («Rfacebook» [^7]), quandl.com («Quandl» [^8]), finance.yahoo.com («quantmod» [^9]) и многих других. Некоторые сайты сами выкладывают API для R.  

\ \ \ \ \ **Пример №2**. Данные из примера №1 по импорту масла в РФ можно получить двумя способами:  

* Сделав запрос к базе UN COMTRADE через форму на веб-странице:  <http://comtrade.un.org/data/>.  

* Воспользовавшись API для R, описание которого приводится по адресу: <http://comtrade.un.org/data/Doc/api/ex/r>.  

\ \ \ \ \ На странице API для R сайта UN COMTRADE приводятся коды пользовательских функций для формирования запроса и примеры использования этих функций. Как и всякий API, этот имеет свои ограничения:  

* Не более одного запроса в секунду с одного IP адреса.  

* Не более 100 запросов в час с одного IP адреса.  

* API находится в режиме разработки и может быть изменён.  

\ \ \ \ \ Используем API UN COMTRADE чтобы извлечь данные, которые лежат в основе первого примера. База данных по умолчанию выдаёт результаты в формате JSON[^10], для работы с которым из R нужна библиотека `rjson` [^11]. Для начала найдём код Российской Федерации в справочнике UN COMTRADE.  


```r
# библиотека для работы с JSON
library('rjson')
# адрес справочника по странам UN COMTRADE
fileURL <- "http://comtrade.un.org/data/cache/partnerAreas.json"
# загружаем данные из формата JSON
reporters <- fromJSON(file = fileURL)
is.list(reporters)
#> [1] TRUE

# соединяем элементы списка построчно
reporters <- t(sapply(reporters$results, rbind))
dim(reporters)
#> [1] 294   2

# превращаем во фрейм
reporters <- as.data.frame(reporters)
head(reporters)
#>    V1                       V2
#> 1 all                      All
#> 2   0                    World
#> 3   4              Afghanistan
#> 4 472 Africa CAMEU region, nes
#> 5   8                  Albania
#> 6  12                  Algeria

# даём столбцам имена
names(reporters) <- c('State.Code', 'State.Name.En')
# находим РФ
reporters[reporters$State.Name.En == 'Russian Federation', ]
#>     State.Code      State.Name.En
#> 219        643 Russian Federation
```

\ \ \ \ \ Итак, код РФ в базе: 643. У базы UN COMTRADE есть ещё одно ограничение: при выборе ежемесячных данных один запрос может охватывать максимум год. Поэтому чтобы собрать данные по месяцам с 2010 по 2018 гг., нужно сделать пять запросов. Для формирования запроса за 2010 год воспользуемся функцией get.Comtrade(), сохранённой по адресу: <https://raw.githubusercontent.com/aksyuk/R-data/master/API/comtrade_API.R>.  


```r
# функция, реализующая API (источник: UN COMTRADE)
source("https://raw.githubusercontent.com/aksyuk/R-data/master/API/comtrade_API.R")
# ежемесячные данные по импорту масла в РФ за 2010 год
# 040510 – код сливочного масла по классификации HS
s1 <- get.Comtrade(r = 'all', p = "643", 
                   ps = as.character(2010), freq = "M",
                   rg = '1', cc = '040510',
                   fmt = "csv")
dim(s1$data)
#> [1] 22 35
is.data.frame(s1$data)
#> [1] TRUE
```

\ \ \ \ \ Код выше загружает в рабочее пространство R функцию get.Comtrade() с известного URL, а затем вызывает её с аргументами:
* `r` -- страны, подавшие отчёт о поставке, 'all' означает выбор всех поставщиков товара в РФ;  

* `p` -- партнёр, в данном случае Российская Федерация;  

* `ps` -- период времени, здесь 2010 год в символьком формате;  

* `freq` -- частота, в данном случае 'M' означает ежемесячные данные;  

* `rg` -- код торгового потока, здесь '1' – это импорт;  

* `cc` -- код товара по гармонизированной классификации, '040510' – сливочное масло;  

* `fmt` – формат, в котором выдаются данные, здесь – «csv».  

\ \ \ \ \ Объект `s1` хранит данные (`s1$fdata`) и результаты проверки запроса (`s1$validation`). Данные представляют собой объект типа `data.frame` с 10 строками и 35 столбцами. Стоит сразу сохранить результаты запроса на диск, чтобы обращаться к ним в любое время без ограничений, которыми обладает API.  


```r
# записываем выборку за 2010 год в файл
write.csv(s1$data, './data/comtrade_2010.csv', row.names = F)
```

\ \ \ \ \ Загрузку данных за все годы можно сделать в цикле. Перебор файлов или URL -- задачи, в которых применение циклов в R оправдано.  


```r
# загрузка данных в цикле
for (i in 2011:2019) {
    # таймер для ограничения API: не более запроса в секунду
    Sys.sleep(5)
    s1 <- get.Comtrade(r = 'all', p = "643", 
                       ps = as.character(i), freq = "M",
                       rg = '1', cc = '040510',
                       fmt="csv")
    # имя файла для сохранения
    file.name <- paste('./data/comtrade_', i, '.csv', 
                       sep = '')
    # записать данные в файл
    write.csv(s1$data, file.name, row.names = F)
    # вывести сообщение в консоль
    print(paste('Данные за', i, 'год сохранены в файл', 
                file.name))
    # сделать запись в лог
    write(paste('Файл', 
                paste('comtrade_', i, '.csv', sep = ''), 
                'загружен', Sys.time()), 
          file = './data/download.log', append = T)
}

```

\ \ \ \ \ Итак, с помощью API базы данных международной торговли UN COMTRADE мы загрузили ежемесячные данные по импорту сливочного масла в РФ и сохранили их в папку «data» внутри рабочей директории в отдельных файлах для каждого года, с 2010 по 2019.  




# Дополнительная информация: парсинг данных с сайтов средствами R     



## Парсинг XML  

\ \ \ \ \ Значительная часть открытых данных в интернете содержится не в файлах .xls или .csv, а в виде таблиц на веб-страницах. Анализ текста веб-страницы с целью извлечь нужную информацию, ориентируясь по тегам разметки, носит название парсинга (от англ. parse -- разбор, структурный анализ). Легче всего понять технологию парсинга веб-страниц в R на примере разбора XML, хотя на практике сайты, написанные на чистом XML, в настоящее время редки.  

\ \ \ \ \ **Пример №4**. Рассмотрим учебную XML-страницу: <http://www.w3schools.com/xml/simple.xml> [^3]. На Рис. 1 показана структура этого файла.  

![Рис.1. Структура XML-файла из примера 2](./pics/pic-01.png)

\ \ \ \ \ Для разбора XML-страниц в R служит пакет «XML»[^4].  


```r
# Загрузка пакетов
library('httr')                # работа с URL по https
library('RCurl')               # работа с URL по http
library('XML')                 # разбор XML-файлов

# адрес XML-страницы
fileURL <- 'https://www.w3schools.com/xml/simple.xml'

# Define certicificate file
cafile <- system.file("CurlSSL", "cacert.pem", package = "RCurl")

# Read page
doc <- GET(fileURL, config(cainfo = cafile))

# разбираем объект как XML
parsedXML <- xmlTreeParse(doc, useInternalNodes = T)
#> No encoding supplied: defaulting to UTF-8.
```

\ \ \ \ \ Итак, в файле содержится информация о меню завтрака, о чём говорит название корневого тега «breakfast menu». По каждой позиции меню, описанной в теге «food», есть название блюда («name»), его цена («price»), описание («description») и количество калорий («calories»). Просмотрев объект `parsedXML`, можно убедиться, что он полностью повторяет эту структуру.    


```r
# просмотр загруженного документа
# ВНИМАНИЕ: не повторять для больших страниц!
parsedXML
#> <?xml version="1.0" encoding="UTF-8"?>
#> <breakfast_menu>
#>   <food>
#>     <name>Belgian Waffles</name>
#>     <price>$5.95</price>
#>     <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description>
#>     <calories>650</calories>
#>   </food>
#>   <food>
#>     <name>Strawberry Belgian Waffles</name>
#>     <price>$7.95</price>
#>     <description>Light Belgian waffles covered with strawberries and whipped cream</description>
#>     <calories>900</calories>
#>   </food>
#>   <food>
#>     <name>Berry-Berry Belgian Waffles</name>
#>     <price>$8.95</price>
#>     <description>Light Belgian waffles covered with an assortment of fresh berries and whipped cream</description>
#>     <calories>900</calories>
#>   </food>
#>   <food>
#>     <name>French Toast</name>
#>     <price>$4.50</price>
#>     <description>Thick slices made from our homemade sourdough bread</description>
#>     <calories>600</calories>
#>   </food>
#>   <food>
#>     <name>Homestyle Breakfast</name>
#>     <price>$6.95</price>
#>     <description>Two eggs, bacon or sausage, toast, and our ever-popular hash browns</description>
#>     <calories>950</calories>
#>   </food>
#> </breakfast_menu>
#> 
```

\ \ \ \ \ Пакет «XML» содержит функции, которые позволяют перемещаться по дереву документа и извлекать текст из тегов с определённым именем и/или атрибутами. Рассмотрим некоторые из них:  

* `xmlTreeParse(`*URL_страницы*`, useInternalNodes = T)` читает структуру XML-страницы, используя её внутреннюю разметку (`useInternalNodes = T`).  

* `xmlRoot(`*документ_XML*`)` возвращает корневую запись (тег) документа. Здесь и далее под документом понимается объект в R, который содержит структуру XML-страницы, прочитанной с помощью функции `xmlTreeParse()`.  

* `xmlName(`*элемент_дерева_XML*`)` возвращает имя тега. Аргумент -- объект типа XMLNode (XML запись).  

* `xmlValue(`*элемент_дерева_XML*`)` возвращает содержимое тега.  


```r
# корневой элемент XML-документа  
rootNode <- xmlRoot(parsedXML)  

# имя корневого тега  
xmlName(rootNode)  
#> [1] "breakfast_menu"

# объект rootNode относится к специальному типу «XML запись»  
class(rootNode)  
#> [1] "XMLInternalElementNode" "XMLInternalNode"        "XMLAbstractNode"

# имена тегов, дочерних к корню (именованный вектор)  
names(rootNode)  
#>   food   food   food   food   food 
#> "food" "food" "food" "food" "food"

# первый элемент дерева (обращаемся как к элементу списка)  
rootNode[[1]]  
#> <food>
#>   <name>Belgian Waffles</name>
#>   <price>$5.95</price>
#>   <description>Two of our famous Belgian Waffles with plenty of real maple syrup</description>
#>   <calories>650</calories>
#> </food>

# первый потомок первого потомка корневого тега...  
rootNode[[1]][[1]]  
#> <name>Belgian Waffles</name>

# ...и его содержимое  
xmlValue(rootNode[[1]][[1]])  
#> [1] "Belgian Waffles"
```

\ \ \ \ \ Код выше реализует своего рода «слепую навигацию» по дереву, когда мы не знаем имён нужных тегов и просто движемся от одного потомка к другому. Для поиска конкретных элементов дерева используем функции:  

* `xmlSApply(`*XML_запись*`, `*имя_функции*`)` применяет («apply») функцию ко всем элементам XML записи.  

* `xpathSApply(`*XML_запись*`, "`*условие*`", `*имя_функции*`)` применяет функцию ко всем элементам, удовлетворяющим условию.  


```r
# извлечь все значения из потомков в XML-записи  
values.all <- xmlSApply(rootNode, xmlValue)  

# просмотреть первые два элемента  
values.all[1:2]  
#>                                                                                                  food 
#>            "Belgian Waffles$5.95Two of our famous Belgian Waffles with plenty of real maple syrup650" 
#>                                                                                                  food 
#> "Strawberry Belgian Waffles$7.95Light Belgian waffles covered with strawberries and whipped cream900"
```

\ \ \ \ \ Функция `xmlSApply()` применила функцию `xmlValue()` ко всем потомкам корневого тега XML-записи и соединила значения входящих в них тегов в одну длинную строку. Затем результат был объединён в вектор, в котором столько же элементов, сколько было тегов «food» под корнем. Определённо, это неудовлетворительный результат, поскольку значения нескольких переменных образовали одно значение.
Пакет «XML» содержит функции, как `xpathSApply()`, поддерживающие XPath (XML Path Language) -- язык запросов к элементам XML-документа. Аргумент, задающий условие на отбор тегов, может использовать, в частности, такие конструкции:  

* `nodename` -- выбрать все записи (теги) с именем «nodename».  

* `/` -- выбрать запись на верхнем уровне иерархии.  

* `//` -- выбрать запись на любом уровне иерархии.  

* `.` -- выбрать текущую запись.  

* `..` -- выбрать родителя текущей записи.  

* `@` -- выбрать атрибуты.  

* `[]` -- в квадратных скобках после имени записи записываются предикаты, т.е. условия на значения.  

* `*` -- выбрать все записи документа.  

* `*@` -- выбрать все записи документа с атрибутами.  

* `node()` -- выбрать все записи всех видов[^5].   

* `text()` -- извлечь значение тега (работает при наличии нескольких пространств имён).   

* `node1/parent::node2` -- выбрать тег с именем «node2», который является родительским по отношению к тегу «node1».  

* `node1/child::node2` -- выбрать тег с именем «node2», который является потомком по отношению к тегу «node1».    

* `node1/following-sibling::node2` -- выбрать тег с именем «node2», который находится на том же уровне иерархии, что и «node1», и следует за ним.  

* `node1/preceding-sibling::node2` -- выбрать тег с именем «node2», который находится на том же уровне иерархии, что и «node1», и следует за ним [^28].   


```r
# вытащить содержимое тегов "name" на любом уровне
xpathSApply(rootNode, "//name", xmlValue)
#> [1] "Belgian Waffles"             "Strawberry Belgian Waffles"  "Berry-Berry Belgian Waffles"
#> [4] "French Toast"                "Homestyle Breakfast"

# вытащить содержимое тегов "price" на любом уровне
xpathSApply(rootNode, "//price", xmlValue)
#> [1] "$5.95" "$7.95" "$8.95" "$4.50" "$6.95"
```

\ \ \ \ \ Пойти дальше, то есть превратить хорошо структурированный файл в объект `data.frame`, поможет функция `xmlToDataFrame(`*XML_запись*`)`:  


```r
# разобрать XML-страницу и собрать данные в таблицу
DF.food <- xmlToDataFrame(rootNode, stringsAsFactors = F)
# предварительный просмотр
dim(DF.food)     # размерность таблицы
#> [1] 5 4

str(DF.food)     # структура (характеристики столбцов)
#> 'data.frame':	5 obs. of  4 variables:
#>  $ name       : chr  "Belgian Waffles" "Strawberry Belgian Waffles" "Berry-Berry Belgian Waffles" "French Toast" ...
#>  $ price      : chr  "$5.95" "$7.95" "$8.95" "$4.50" ...
#>  $ description: chr  "Two of our famous Belgian Waffles with plenty of real maple syrup" "Light Belgian waffles covered with strawberries and whipped cream" "Light Belgian waffles covered with an assortment of fresh berries and whipped cream" "Thick slices made from our homemade sourdough bread" ...
#>  $ calories   : chr  "650" "900" "900" "600" ...
```

\ \ \ \ \ Напомним, что аргумент `stringsAsFactors = F` запрещает кодировать символьные переменные в факторы. Структура документа из этого примера слишком проста, чтобы проверить работу более сложных запросов.  

\ \ \ \ \ **Пример №3**. Приведём пример использования синтаксиса XPath. Используем курсы обмена евро на другие валюты, устанавливаемые на дату, которые публикуются Европейским центральным банком на сайте <https://www.ecb.europa.eu/stats/eurofxref/eurofxref-daily.xml>. Статистика доступна в формате XML. На Рис. 2 показана структура страницы с курсами на последнюю установленную дату.

![Рис.2. Структура XML-файла из примера 3](./pics/pic-02.png)


```r
# обменный курс евро по отношению к иностранным
#  валютам, на текущую дату
fileURL <- 'https://www.ecb.europa.eu/stats/eurofxref/eurofxref-daily.xml'
# xmlParse() не работает с https, поэтому сначала читаем страницу как текст
xmlData <- getURL(fileURL)
# и разбираем содержимое в объект doc
parsedXML <- xmlParse(xmlData, useInternalNodes = T)

# корневой элемент
rootNode <- xmlRoot(parsedXML)
# класс объекта rootNode
class(rootNode)
#> [1] "XMLInternalElementNode" "XMLInternalNode"        "XMLAbstractNode"
# имя корневого элемента
xmlName(rootNode)
#> [1] "Envelope"
```

\ \ \ \ \ Структура этого дерева XML проста, но содержит пространство имён, что может доставить сложности при разборе. Запись корневого тега как `<gesmes:Envelope>` означает, что тег называется "Envelope", но имя задано на пространстве имён "gesmes", ссылка на которое дана в шапке файла. Сами курсы записаны тегами "Cube", причём и название валюты, и обменный курс записаны в атрибутах ("currency" и "rate" соответственно). Для начала попробуем извлечь все уникальные имена тегов документа, используя XPath-запрос.  


```r
# вытаскиваем имена всех тегов документа (*)
#  на любом уровне иерархии (//)

tag <- xpathSApply(rootNode, "//*", xmlName)
# оставляем только уникальные
tag <- unique(tag)

# считаем их количество
length(tag)
#> [1] 5

# смотрим названия
tag
#> [1] "Envelope" "subject"  "Sender"   "name"     "Cube"
```

\ \ \ \ \ Посмотрим, как обращаться к тегу с явно заданным пространством имён.    


```r
# в документе есть теги с явно объявленным пространством имён (namespace) gesmes
try.tag <- xpathSApply(rootNode, "//name", xmlValue)         # пусто
try.tag
#> list()
try.tag <- xpathSApply(rootNode, "//gesmes:name", xmlValue)  # тег найден
try.tag
#> [1] "European Central Bank"
```

\ \ \ \ \ Пространство имён в файле может быть не одно. Посмотрим все пространства документа.    


```r
# посмотреть пространство имён xml-документа
xmlNamespace(rootNode)
#>                                 gesmes 
#> "http://www.gesmes.org/xml/2002-08-01" 
#> attr(,"class")
#> [1] "XMLNamespace"
```


\ \ \ \ \ Теперь извлечём наименования валют и обменные курсы из атрибутов тегов "Cube", которые содержат атрибут "currency".  



```r
# информация о курсах записана в тегах Cube без явного namespace
#  поэтому используем функцию name() для поиска тега
# source: https://stackoverflow.com/questions/45634155/parse-nested-xml-with-namespaces-in-r
tag <- xpathSApply(rootNode, "//*[name()='Cube'][@currency]", xmlGetAttr, 'currency')
tag
#>  [1] "USD" "JPY" "BGN" "CZK" "DKK" "GBP" "HUF" "PLN" "RON" "SEK" "CHF" "ISK" "NOK" "HRK" "RUB"
#> [16] "TRY" "AUD" "BRL" "CAD" "CNY" "HKD" "IDR" "ILS" "INR" "KRW" "MXN" "MYR" "NZD" "PHP" "SGD"
#> [31] "THB" "ZAR"
curr.names <- unlist(tag)

# курсы обмена
tag <- xpathSApply(rootNode, "//*[name()='Cube'][@currency]", xmlGetAttr, 'rate')
tag
#>  [1] "1.1767"   "129.60"   "1.9558"   "25.546"   "7.4366"   "0.85718"  "349.18"   "4.5779"  
#>  [9] "4.9318"   "10.2413"  "1.0789"   "148.60"   "10.3615"  "7.4885"   "87.0756"  "9.8450"  
#> [17] "1.6206"   "6.1506"   "1.4838"   "7.6285"   "9.1625"   "16996.28" "3.7932"   "87.2340" 
#> [25] "1373.36"  "23.9001"  "4.9363"   "1.6882"   "58.803"   "1.5920"   "38.525"   "17.5420"
curr.rate <- unlist(tag)
```

\ \ \ \ \ Обязательно нужно зафиксировать дату, она записана в теге «time» тега «Cube».   


```r
# дата (обращаемся только к тегу Cube, в котором есть атрибут time)
tag <- xpathSApply(rootNode, "//*[name()='Cube'][@time]", xmlGetAttr, 'time')
tag
#> [1] "2021-08-26"
```

\ \ \ \ \ Записываем всё во фрейм.    


```r
# превращаем XML во фрейм
DF.EUR <- cbind(curr.names, curr.rate, tag)
colnames(DF.EUR) <- c('Валюта', 'Курс к евро', 'Дата')
# предварительный просмотр
dim(DF.EUR)     # размерность таблицы
#> [1] 32  3
str(DF.EUR)     # структура (характеристики столбцов)
#>  chr [1:32, 1:3] "USD" "JPY" "BGN" "CZK" "DKK" "GBP" "HUF" "PLN" "RON" "SEK" "CHF" "ISK" ...
#>  - attr(*, "dimnames")=List of 2
#>   ..$ : NULL
#>   ..$ : chr [1:3] "Валюта" "Курс к евро" "Дата"
```



## Парсинг HTML  

\ \ \ \ \ **Пример №5**. Технически разбор HTML-страниц мало чем отличается от разбора XML, поскольку здесь используются те же функции пакета XML и язык XPath. Однако извлекать данные из HTML обычно труднее из-за большого количества не относящейся к данным информации как в содержимом страницы, так и в разметке. Разберём страницу с топ-200 книг по версии BBC, размещённый на Википедии.   

\ \ \ \ \ В коде ниже есть закомментированные строки кода, которые сработают в Linux, но вызывают ошибку в Windows. Это связано с тем, что пакет `RCurl` содержит ошибки, которые на момент написания этого руководства не были исправлены. Под Windows для загрузки данных по протоколу https рекомендуется использовать пакет `httr`.   

\ \ \ \ \ Ещё одна проблема связана с распознаванием кириллицы. Кодировка не всегда интерпретируется верно, поэтому надёжнее строить запросы на отбор тегов, по возможности без использования кириллицы.   


```r

# пакет, который позволяет загружать данные по протоколу https под Windows
library('httr')

# URL страницы топ-200 книг по версии BBC на Википедии
fileURL <- "https://ru.wikipedia.org/wiki/200_%D0%BB%D1%83%D1%87%D1%88%D0%B8%D1%85_%D0%BA%D0%BD%D0%B8%D0%B3_%D0%BF%D0%BE_%D0%B2%D0%B5%D1%80%D1%81%D0%B8%D0%B8_%D0%91%D0%B8-%D0%B1%D0%B8-%D1%81%D0%B8"

# загружаем текст html-страницы, явно указывая кодировку (помогает от проблем с кириллицей)
# html <- getURL(fileURL, .encoding = 'UTF-8')  # работает под Linux
html <- GET(fileURL)     # работает под Windows

# класс объекта с загруженным содержимым
class(html)
#> [1] "response"

# дальше только для функции GET()
html <- content(html, 'text', encoding = 'UTF-8')
class(html)
#> [1] "character"

# разбираем как html
# parsedHTML <- htmlParse(html)                      # для getURL()
parsedHTML <- htmlParse(html, useInternalNodes = T)  # для GET()

# корневой элемент
rootNode <- xmlRoot(parsedHTML)
```

\ \ \ \ \ Далее, чтобы определить, в каких тегах содержится нужная информация, необходимо изучить исходный код страницы. Браузеры «Mozilla Firefox» и «Chrome» позволяют просматривать отдельно исходный код выделенного фрагмента. Для этого нужно выделить интересующую часть текста, кликнуть на ней правой кнопкой мыши и выбрать пункт контекстного меню «Исходный код выделенного фрагмента» или «Просмотреть код». Просмотр кода страницы позволил определить, что названия книг -- это значения атрибутов «title» тегов «a», которые находятся внутри тегов «li» после заголовка (`<h2...><span...>Список</span></div>`) с текстом «Список» (Рис. 3). У нужного нам тега «span» есть атрибут «class», равный "mw-headline" -- используем это, чтобы не применять кириллицу в xpath-запросе. Чтобы вытащить нужные теги «a», необходимо найти «span» с атрибутом «class», равным "mw-headline", подняться от него к тегу «h2» на уровень вверх, затем выбрать следующий тег «div» на том же уровне, и найти всех его потомков «li/a». Причём для каждой книги есть две гиперссылки, первая для названия (`a[1]`), вторая для автора (`a[2]`).   

\ \ \ \ \ Выберем все названия и авторов, а также ссылки на статьи с описанием каждой книги, кроме тех случаев, когда такой страницы нет.    

![Рис.3. Структура html-страницы из примера 4](./pics/pic-03.png)  


```r
# выбираем все наименования книг
wiki.title <- xpathSApply(rootNode, '//span[@class="mw-headline"][1]/parent::h2/following-sibling::div//li/*[1]',
                          xmlGetAttr, 'title')
# проверяем длину
length(wiki.title)
#> [1] 201

# превращаем в вектор
wiki.title[sapply(wiki.title, is.null)] <- NA
wiki.title <- unlist(wiki.title)

# просмотр первых трёх элементов вектора
wiki.title[1:3]
#> [1] "Р’Р»Р°СЃС‚РµР»РёРЅ РєРѕР»РµС†"                 
#> [2] "Р“РѕСЂРґРѕСЃС‚СЊ Рё РїСЂРµРґСѓР±РµР¶РґРµРЅРёРµ"
#> [3] "РўС‘РјРЅС‹Рµ РЅР°С‡Р°Р»Р°"

# исправляем кодировку
Encoding(wiki.title) <- 'UTF-8'
wiki.title[1:3]
#> [1] "Властелин колец"          "Гордость и предубеждение" "Тёмные начала"

# выбираем всех авторов книг
wiki.author <- xpathSApply(rootNode, '//span[@class="mw-headline"][1]/parent::h2/following-sibling::div//li/*[2]',
                           xmlGetAttr, 'title')
# проверяем длину
length(wiki.author)
#> [1] 201

# превращаем в вектор
wiki.author[sapply(wiki.author, is.null)] <- NA
wiki.author <- unlist(wiki.author)

# исправляем кодировку
Encoding(wiki.author) <- 'UTF-8'
wiki.author[1:3]
#> [1] "Толкин, Джон Рональд Руэл" "Остин, Джейн"              "Пулман, Филип"

# выбираем все ссылки на книги
wiki.link <- xpathSApply(rootNode, '//span[@class="mw-headline"][1]/parent::h2/following-sibling::div//li/*[1]',
                         xmlGetAttr, 'href')
# проверяем длину
length(wiki.link)
#> [1] 201
# просмотр первых трёх элементов вектора
wiki.link[1:3]
#> [[1]]
#> [1] "/wiki/%D0%92%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D0%BB%D0%B8%D0%BD_%D0%BA%D0%BE%D0%BB%D0%B5%D1%86"
#> 
#> [[2]]
#> [1] "/wiki/%D0%93%D0%BE%D1%80%D0%B4%D0%BE%D1%81%D1%82%D1%8C_%D0%B8_%D0%BF%D1%80%D0%B5%D0%B4%D1%83%D0%B1%D0%B5%D0%B6%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5"
#> 
#> [[3]]
#> [1] "/wiki/%D0%A2%D1%91%D0%BC%D0%BD%D1%8B%D0%B5_%D0%BD%D0%B0%D1%87%D0%B0%D0%BB%D0%B0"

# превращаем в вектор
wiki.link[sapply(wiki.link, is.null)] <- NA
wiki.link <- unlist(wiki.link)

# добавляем адрес сайта во внутренние ссылки
wiki.link[!is.na(wiki.link)] <- paste0('https://ru.wikipedia.org', 
                                       wiki.link[!is.na(wiki.link)])

# объединяем во фрейм
DF.wiki <- data.frame(Книга = wiki.title, Автор = wiki.author, Ссылка = wiki.link)

# отбрасываем полностью пустые строки
DF.wiki <- DF.wiki[rowSums(is.na(DF.wiki)) != ncol(DF.wiki), ]

# записываем в файл .csv
write.csv(DF.wiki, file = './data/DF_wiki.csv', row.names = F)
# сделать запись в лог
write(paste('Файл "DF_wiki.csv" записан', Sys.time()), 
      file = log.filename, append = T)
```

\ \ \ \ \ В этом примере мы столкнулись с неверным разбором символов кириллицы. Пакет `rvest`, предназначенный для более удобного сбора данных с html-страниц, позволяет решить проблему кодировок в большинстве случаев.      



## Веб-скраппинг с пакетом "rvest"  

\ \ \ \ \ Термин "веб-скраппинг" (Web Scraping) понемногу входит в обиход специалистов по данным, заменяя понятие "парсинг веб-страниц". Скраппинг означает именно анализ сайтов с целью сбора статистики, в то время как парсинг -- более общий процесс анализа структуры текста. Сбор данных с сайтов можно производить самыми разными способами:  

* **Ручная копипаста** -- способ медленный, но устойчивый к различным вариациям структуры сайтов. Человеку, с одной стороны, проще понять, какие сведения со страницы требуется собрать. С другой стороны, при больших объёмах работы неизбежны случайные ошибки, а производительность самая здесь низкая.  

* **Поиск по текстовым шаблонам** -- другой простойи мощный подход к извлечению информации из интернета. Применяются регулярные выражения языков программирования. Методы XPath, рассмотренные выше, как раз из этой серии.  

* **Использование API** возможно для большинства крупных платформ: Facebook, Twitter, LinkedIn и других. Минус в том, что бесплатные версии API обычно сильно ограничивают объём данных, доступных для скачивания в единицу времени.   

* **Парсинг DOM** (Document Object Model -- «объектная модель документа»). Используя браузеры, программы могут извлекать динамический контент, генерируемый на стороне клиента. Также можно анализировать веб-страницы с помощью дерева объектов DOM [^27]. В R такие возиожности реализованы в пакете `rvest`.     

\ \ \ \ \ **Пример №6**. Мы будем собирать данные о самых популярных фильмах 2016 года выпуска по версии IMDB, и для поиска нужных объектов на сайте нам понадобится свободная программа "Selector Gadget" (<https://selectorgadget.com/>), доступная в виде расширения к Chrome.  


```r
# загружаем пакеты
library('rvest')     # работа с DOM сайта
library('dplyr')     # инструменты трансформирования данных

# URL страницы для скраппинга
url <- 'http://www.imdb.com/search/title?count=100&release_date=2016,2016&title_type=feature'

# читаем HTML страницы
webpage <- read_html(url)
```

\ \ \ \ \ Теперь соберём со страницы следующие данные:  

* `Rank` -- Ранг фильма от 1 до 100 в списке самых популярных фильмов 2016 года выпуска.  

* `Title` -- название фильма;  

* `Description` -- описание фильма;  

* `Runtime` -- длительность фильма;  

* `Genre` -- жанр фильма; в случае если их несколько, берём первый;  

* `Metascore` -- метаоценка сайта IMDB фильма.   

\ \ \ \ \ На Рис.4 показан скриншот страницы.  

![Рис.4. Вид веб-страницы из примера 6](./pics/pic-04.png) 

\ \ \ \ \ Начнём с поля `Rank`. Используем расширение "Selector Gadget", чтобы получить CSS-селектор для тега, в котором записан ранг фильма. Для этого нажмите сначала на иконку расширения справа от адресной строки браузера, а затем на ранг фильма (Рис.5). Убедитесь, что все ранги подсвечены жёлтым. В рамке внизу страницы появится искомый селектор.  

![Рис.5. Поиск селектора для ранга фильма с помощью Selector Gadget](./pics/pic-05.png) 

\ \ \ \ \ Теперь используем этот селектор для выбора всех объектов, содержащих ранги, на странице. Обратите внимание, как мы объединяем две функции в одну строку с помощью пайплайна `%>%`. Объект, который возвращает функция `html_nodes()`, подаётся на вход функции `html_text()`, а результат записывается в переменную `rank_data`.  


```r
# скраппим страницу по селектору и преобразуем в текст
rank_data <- webpage %>% html_nodes('.text-primary') %>% html_text

# размер вектора
length(rank_data)
#> [1] 100

# первые шесть рангов
head(rank_data) 
#> [1] "1." "2." "3." "4." "5." "6."
```

\ \ \ \ \ Для рангов предпочтителен числовой формат, поэтому преобразуем полученные данные с помощью функции `as.numeric()`.  


```r
# конвертируем ранги в числовые данные
rank_data <- as.numeric(rank_data)

# результат
head(rank_data)
#> [1] 1 2 3 4 5 6
```

\ \ \ \ \ Теперь точно так же найдём селектор для названий фильмов (Рис.6). Чтобы выбрать только названия фильмов (они подсвечены жёлтым), здесь требуется творческий подход: необходимо выбрать первый заголовок, а затем кликнуть по сортировке "A-Z", чтобы отсечь все лишние ссылки. Применим селектор `'.lister-item-header a'` для загрузки названий.  

![Рис.6. Поиск селектора для названия фильма](./pics/pic-06.png) 


```r
# отбор названий фильмов по селектору
title_data <- webpage %>% html_nodes('.lister-item-header a') %>% html_text

# результаты
length(title_data)
#> [1] 100
head(title_data)
#> [1] "Отряд самоубийц"      "Не дыши"              "Великолепная семерка" "Дэдпул"              
#> [5] "Расплата"             "Зверопой"
```

\ \ \ \ \ Код ниже отвечает за скраппинг остальных полей, кроме сводного рейтинга (`Metascore`).   


```r
# описания фильмов
description_data <- webpage %>% html_nodes('.ratings-bar+ .text-muted') %>% 
  html_text()
length(description_data)
#> [1] 100
head(description_data)
#> [1] "\nA secret government agency recruits some of the most dangerous incarcerated super-villains to form a defensive task force. Their first mission: save the world from the apocalypse."                                          
#> [2] "\nHoping to walk away with a massive fortune, a trio of thieves break into the house of a blind man who isn't as helpless as he seems."                                                                                         
#> [3] "\nSeven gunmen from a variety of backgrounds are brought together by a vengeful young widow to protect her town from the private army of a destructive industrialist."                                                          
#> [4] "\nA wisecracking mercenary gets experimented on and becomes immortal but ugly, and sets out to track down the man who ruined his looks."                                                                                        
#> [5] "\nAs a math savant uncooks the books for a new client, the Treasury Department closes in on his activities, and the body count starts to rise."                                                                                 
#> [6] "\nIn a city of humanoid animals, a hustling theater impresario's attempt to save his theater with a singing competition becomes grander than he anticipates even as its finalists find that their lives will never be the same."

# длительности фильмов
runtime_data <- webpage %>% html_nodes('.text-muted .runtime') %>% html_text
length(runtime_data)
#> [1] 100
head(runtime_data)
#> [1] "123 min" "88 min"  "132 min" "108 min" "128 min" "108 min"

# жанры фильмов 
genre_data <- webpage %>% html_nodes('.genre') %>% html_text
length(genre_data)
#> [1] 100
head(genre_data)
#> [1] "\nAction, Adventure, Fantasy            " "\nCrime, Horror, Thriller            "   
#> [3] "\nAction, Adventure, Western            " "\nAction, Adventure, Comedy            " 
#> [5] "\nAction, Crime, Drama            "       "\nAnimation, Comedy, Family            "
```

\ \ \ \ \ С полем `Metascore` возникает проблема, поскольку оно есть только у 97 фильмов из 100.  


```r
# селектор для общего рейтинга (метарейтинга) 
metascore_data <- webpage %>% html_nodes('.ratings-metascore') %>% html_text
# предварительный результат
length(metascore_data)
#> [1] 97
```

\ \ \ \ \ Проблему решает пользовательская функция, которая работает по принципу перебора всех тегов, в которые вложен `.ratings-metascore`; анализ страницы показал, что это теги `.lister-item-content`. Функция `html_nodes()` возвращает строку нулевой длины, когда не находит искомый тег внутри заданного, и нам нужно только отловить все эти случаи и заменить их на `NA`.    


```r
# функция перебора тегов внутри тегов более высокого уровня
get_tags <- function(node){
  # найти все теги с метарейтингом
  raw_data <- html_nodes(node, selector) %>% html_text
  # значения нулевой длины (для фильма нет такого тега) меняем на пропуски
  data_NAs <- ifelse(length(raw_data) == 0, NA, raw_data)
}

# это глобальная переменная будет неявно передана функции get_tags()
selector <- '.ratings-metascore'
# находим все ноды (теги) верхнего уровня, с информацией о каждом фильме
doc <- html_nodes(webpage, '.lister-item-content')
# применяем к этим тегам поиск метарейтинга и ставим NA там, где тега нет
metascore_data <- sapply(doc, get_tags)
# предварительный результат
length(metascore_data)
#> [1] 100
head(metascore_data)
#> [1] "\n40        \n        Metascore\n            "
#> [2] "\n71        \n        Metascore\n            "
#> [3] "\n54        \n        Metascore\n            "
#> [4] "\n65        \n        Metascore\n            "
#> [5] "\n51        \n        Metascore\n            "
#> [6] "\n59        \n        Metascore\n            "
```

  
\ \ \ \ \ Совместим данные в один фрейм и запишем его в файл `.csv`. В следующей практике мы вернёмся к этой таблице, чтобы почистить столбцы от лишних символов.     


```r
# совмещаем данные в один фрейм
DF_movies_short <- data.frame(Rank = rank_data, Title = title_data, 
                              Description = description_data, 
                              Runtime = runtime_data, 
                              Genre = genre_data,  Metascore = metascore_data)
# результат
dim(DF_movies_short)
#> [1] 100   6
str(DF_movies_short)
#> 'data.frame':	100 obs. of  6 variables:
#>  $ Rank       : num  1 2 3 4 5 6 7 8 9 10 ...
#>  $ Title      : chr  "Отряд самоубийц" "Не дыши" "Великолепная семерка" "Дэдпул" ...
#>  $ Description: chr  "\nA secret government agency recruits some of the most dangerous incarcerated super-villains to form a defensiv"| __truncated__ "\nHoping to walk away with a massive fortune, a trio of thieves break into the house of a blind man who isn't a"| __truncated__ "\nSeven gunmen from a variety of backgrounds are brought together by a vengeful young widow to protect her town"| __truncated__ "\nA wisecracking mercenary gets experimented on and becomes immortal but ugly, and sets out to track down the m"| __truncated__ ...
#>  $ Runtime    : chr  "123 min" "88 min" "132 min" "108 min" ...
#>  $ Genre      : chr  "\nAction, Adventure, Fantasy            " "\nCrime, Horror, Thriller            " "\nAction, Adventure, Western            " "\nAction, Adventure, Comedy            " ...
#>  $ Metascore  : chr  "\n40        \n        Metascore\n            " "\n71        \n        Metascore\n            " "\n54        \n        Metascore\n            " "\n65        \n        Metascore\n            " ...

# записываем в .csv
write.csv(DF_movies_short, file = './data/DF_movies_short.csv', row.names = F)
# сделать запись в лог
write(paste('Файл "DF_movies_short.csv" записан', Sys.time()), 
      file = log.filename, append = T)
```



## Загрузка данных из других форматов  


\ \ \ \ \ Не будет преувеличением сказать, что в настоящее время в R реализовано чтение данных из всех популярных форматов. Мы ограничимся рассмотренными выше примерами, однако перечислим ещё несколько форматов и названия пакетов R для их обработки.    

* Книги Ms Excel .xls, .xlsx – пакет «xlsx» [^12].  

* Электронные таблицы Open Office Calc в формате .ods – пакет «readODS» [^13].  

* Файлы данных SPSS, SAS, Octave, Minitab, Stata, Weka – пакет «foreign» [^14].  

* Базы данных MySQL – пакет «RMySQL» [^15].  

* Базы данных PostgreQL, Microsoft Access, MySQL, SQLite – пакет «RODBC» [^16].  

* База данных документов MongoDB – пакет «RMongo» [^17].  

* Shapefiles и другие форматы представления геоданных – пакеты «shapefiles» [^18], «rdgal» [^19], «rgeos» [^20], «raster» [^21].  

* Изображения в форматах .jpeg, .png, .bmp – пакеты «jpeg» [^22], «png» [^23], «readbitmap» [^24].  

* Чтение и визуализация аудио – пакеты «tuneR» [^25], «seewave» [^26].  


[^1]: R Core Team (2015). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL <https://www.R-project.org/>  

[^2]: Hadley Wickham. Tidy data. The Journal of Statistical Software, vol. 59, 2014.  
[^3]: Jeffrey Leek. Материалы курса «Getting and Cleaning Data» Университета Джонса Хопкинса на портале coursera.org, доступные в репозитории на github.com: <https://github.com/jtleek/modules/tree/master/03_GettingData>.  

[^4]: Duncan Temple Lang and the CRAN Team (2015). XML: Tools for Parsing and Generating XML Within R and S-Plus. R package version 3.98-1.3. <https://CRAN.R-project.org/package=XML>. 

[^5]: XPath Syntax. URL: <http://www.w3schools.com/xsl/xpath_syntax.asp>.  

[^6]: Jeff Gentry (2015). twitteR: R Based Twitter Client. R package version 1.1.9. <https://CRAN.R-project.org/package=twitteR>.  

[^7]: Pablo Barbera and Michael Piccirilli (2015). Rfacebook: Access to Facebook API via R. R package version 0.6. <https://CRAN.R-project.org/package=Rfacebook>.  

[^8]: Raymond McTaggart, Gergely Daroczi and Clement Leung (2015). Quandl: API Wrapper for Quandl.com. R package version 2.7.0. <https://CRAN.R-project.org/package=Quandl>.  

[^9]: Jeffrey A. Ryan (2015). quantmod: Quantitative Financial Modelling Framework. R package version 0.4-5. <https://CRAN.R-project.org/package=quantmod>.  

[^10]: JSON (JavaScript Object Notation) – простой формат обмена данными, удобный для чтения и написания как человеком, так и компьютером.  

[^11]: Введение в JSON. URL: <http://www.json.org/json-ru.html>.  

[^12]: Adrian A. Dragulescu (2014). xlsx: Read, write, format Excel 2007 and Excel 97/2000/XP/2003 files. R package version 0.5.7. <https://CRAN.R-project.org/package=xlsx>.  

[^13]: Gerrit-Jan Schutten (2014). readODS: Read ODS files and puts them into data frames. R package version 1.4. <https://CRAN.R-project.org/package=readODS>.  

[^14]: R Core Team (2015). foreign: Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, Weka, dBase, .... R package version 0.8-66. <https://CRAN.R-project.org/package=foreign>.  

[^15]: Jeroen Ooms, David James, Saikat DebRoy, Hadley Wickham and Jeffrey Horner (2015). RMySQL: Database Interface and 'MySQL' Driver for R. R package version 0.10.7. <https://CRAN.R-project.org/package=RMySQL>.

[^16]: Brian Ripley and Michael Lapsley (2015). RODBC: ODBC Database Access. R package version 1.3-12. <https://CRAN.R-project.org/package=RODBC>.  

[^17]: Tommy Chheng (2013). RMongo: MongoDB Client for R. R package version 0.0.25. <https://CRAN.R-project.org/package=RMongo>.  

[^18]: Ben Stabler (2013). shapefiles: Read and Write ESRI Shapefiles. R package version 0.7. <https://CRAN.R-project.org/package=shapefiles>.  

[^19]: Roger Bivand, Tim Keitt and Barry Rowlingson (2015). rgdal: Bindings for the Geospatial Data Abstraction Library. R package version 1.1-3. <https://CRAN.R-project.org/package=rgdal>.  

[^20]: Roger Bivand and Colin Rundel (2015). rgeos: Interface to Geometry Engine -- Open Source (GEOS). R package version 0.3-15. <https://CRAN.R-project.org/package=rgeos>.  

[^21]: Robert J. Hijmans (2015). raster: Geographic Data Analysis and Modeling. R package version 2.5-2. <https://CRAN.R-project.org/package=raster>.  

[^22]: Simon Urbanek (2014). jpeg: Read and write JPEG images. R package version 0.1-8. <https://CRAN.R-project.org/package=jpeg>.  

[^23]: Simon Urbanek (2013). png: Read and write PNG images. R package version 0.1-7. <https://CRAN.R-project.org/package=png>.  

[^24]: Gregory Jefferis (2014). readbitmap: Simple Unified Interface to Read Bitmap Images (BMP,JPEG,PNG). R package version 0.1-4. <https://CRAN.R-project.org/package=readbitmap>.  

[^25]: Uwe Ligges, Sebastian Krey, Olaf Mersmann, and Sarah Schnackenberg (2013). tuneR: Analysis of music. URL: <http://r-forge.r-project.org/projects/tuner/>.  

[^26]: Sueur J., Aubin T., Simonis C. (2008). Seewave: a free modular tool for sound analysis and synthesis. Bioacoustics, 18: 213-226. URL: <https://CRAN.R-project.org/package=seewave>.  

[^27]: Saurav Kaushik (MARCH 27, 2017). Beginner’s Guide on Web Scraping in R (using rvest) with hands-on example. URL: <https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/>.    

[^28]: XSLT: Применение осей. URL: <https://xsltdev.ru/xslt/recipes/primenenie-osey/>   
